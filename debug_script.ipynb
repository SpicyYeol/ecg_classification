{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load package",
   "id": "c1ebd09997963749"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-12T06:58:05.917166Z",
     "start_time": "2024-09-12T06:58:03.522167Z"
    }
   },
   "source": [
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from torch.cuda import device\n",
    "from torch.utils.data import DataLoader, random_split"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T09:04:58.917369Z",
     "start_time": "2024-09-12T09:04:58.902369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N_DATA = [2]\n",
    "OFFSET = None\n",
    "DEBUG = False\n",
    "MODEL_TYPES = [1]  # , 2, 3, 4, 11, 12, 13, 14]#, 15, 16]\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "PLOT = False\n",
    "SAVE_CAM = True  # Save CAM images\n",
    "CAM_DIR = 'cam_images'  # Directory to save CAM images\n",
    "os.makedirs(CAM_DIR, exist_ok=True)\n",
    "dtype = 1"
   ],
   "id": "6fe3389dd9eaec6b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### analysis.py load_and_preprocess_data",
   "id": "76c3db3708e03fdc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T09:07:18.541536Z",
     "start_time": "2024-09-12T09:07:18.509535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data_source_utils import load_dataset, process_dataset\n",
    "from preprocess_utils import preprocess_dataset\n",
    "from analysis import load_json_file"
   ],
   "id": "99c67fc3a387e243",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T09:06:13.003380Z",
     "start_time": "2024-09-12T09:06:12.995380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_list_path = \"F:\\homes\\data_list.txt\"\n",
    "\n",
    "chunk_file_name_list = []\n",
    "dataset_name = []\n",
    "\n",
    "with open(data_list_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 각 줄을 공백으로 분리\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:  # 앞부분과 뒷부분이 존재하는 경우\n",
    "            chunk_file_name_list.append(parts[0])  # 앞부분 저장\n",
    "            dataset_name.append(int(parts[1]))  # 뒷부분 저장\n",
    "\n",
    "# n_data에서 dataset_name과 중복된 항목 제거\n",
    "n_data = [n for n in N_DATA if n not in dataset_name]\n",
    "\n",
    "preprocessed_dataset = []\n",
    "dataset = load_dataset(offset=OFFSET, n_data=n_data)\n",
    "if len(dataset) > 0:\n",
    "    for data in dataset:\n",
    "        preprocessed_dataset.append(preprocess_dataset(data['data'], dtype, data['fs'], plot=PLOT, debug=DEBUG))"
   ],
   "id": "a586f4fcb9e381cc",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T09:06:58.607798Z",
     "start_time": "2024-09-12T09:06:58.598798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_files = []\n",
    "for chunk_file_name in chunk_file_name_list:\n",
    "    file_pattern = \"F:/homes/preprocessed_data/preprocessed_data_\" + chunk_file_name + \"_*.json\"\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    all_files.extend(file_list)  # 모든 파일을 리스트에 추가"
   ],
   "id": "71331dbd85c30c21",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T09:07:41.401654Z",
     "start_time": "2024-09-12T09:07:21.320628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 병렬로 파일 로드\n",
    "if all_files:\n",
    "    loaded_data = Parallel(n_jobs=-1)(delayed(load_json_file)(file_name) for file_name in all_files)\n",
    "\n",
    "    # 병렬 로드된 데이터를 하나의 리스트로 확장\n",
    "    for data in loaded_data:\n",
    "        preprocessed_dataset.extend(data)"
   ],
   "id": "6a31fa55e2bfce45",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:19:58.131218Z",
     "start_time": "2024-09-12T11:19:58.127218Z"
    }
   },
   "cell_type": "code",
   "source": "len(preprocessed_dataset)",
   "id": "a79025101c4bf64e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8528"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:29:28.412680Z",
     "start_time": "2024-09-12T11:29:28.407680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_min_max_data_length(preprocessed_dataset):\n",
    "    # 'data' 필드의 길이를 저장할 리스트\n",
    "    data_lengths = [len(item['data']) for item in preprocessed_dataset if 'data' in item]\n",
    "\n",
    "    if data_lengths:\n",
    "        min_length = min(data_lengths)\n",
    "        max_length = max(data_lengths)\n",
    "        return min_length, max_length\n",
    "    else:\n",
    "        raise ValueError(\"The dataset does not contain 'data' field or is empty.\")"
   ],
   "id": "e468cb9f95be7b04",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "preprocessed_dataset[0]",
   "id": "e7da1c6801749b67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:29:46.533045Z",
     "start_time": "2024-09-12T11:29:46.515045Z"
    }
   },
   "cell_type": "code",
   "source": "min, max = get_min_max_data_length(preprocessed_dataset)",
   "id": "62484447915c123c",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mmin\u001B[39m, \u001B[38;5;28mmax\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mget_min_max_data_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocessed_dataset\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[24], line 6\u001B[0m, in \u001B[0;36mget_min_max_data_length\u001B[1;34m(preprocessed_dataset)\u001B[0m\n\u001B[0;32m      3\u001B[0m data_lengths \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mlen\u001B[39m(item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m preprocessed_dataset \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m item]\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data_lengths:\n\u001B[1;32m----> 6\u001B[0m     min_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mmin\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_lengths\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     max_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(data_lengths)\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m min_length, max_length\n",
      "\u001B[1;31mTypeError\u001B[0m: 'int' object is not callable"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:29:54.258478Z",
     "start_time": "2024-09-12T11:29:54.254478Z"
    }
   },
   "cell_type": "code",
   "source": "min,max",
   "id": "8b4cd34db6b42dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1031, 6948)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:40:11.427398Z",
     "start_time": "2024-09-12T11:40:11.422398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def segment_ecg(data, sampling_rate=114, window_duration=3, overlap_duration=0.5):\n",
    "    \"\"\"\n",
    "    ECG 데이터를 주어진 윈도우 크기와 겹침을 고려하여 분할하는 함수.\n",
    "    \n",
    "    :param data: ECG 데이터 (1D array 또는 list)\n",
    "    :param sampling_rate: 샘플링 속도 (Hz)\n",
    "    :param window_duration: 윈도우 크기 (초) - 최소 3개의 ECG 파형을 포함할 수 있는 크기\n",
    "    :param overlap_duration: 겹침 크기 (초) - 0.5초씩 겹침\n",
    "    :return: 분할된 ECG 데이터 세그먼트 리스트\n",
    "    \"\"\"\n",
    "    # 윈도우 크기와 겹침 크기를 샘플 단위로 변환\n",
    "    window_size = int(window_duration * sampling_rate)  # 3초에 해당하는 샘플 개수\n",
    "    overlap_size = int(overlap_duration * sampling_rate)  # 0.5초에 해당하는 샘플 개수\n",
    "\n",
    "    # 분할된 데이터를 저장할 리스트\n",
    "    segmented_data = []\n",
    "\n",
    "    # 슬라이딩 윈도우로 데이터 분할\n",
    "    start = 0\n",
    "    while start + window_size <= len(data):\n",
    "        end = start + window_size\n",
    "        segmented_data.append(data[start:end])\n",
    "        start += window_size - overlap_size  # 겹침을 고려한 다음 윈도우의 시작 지점\n",
    "\n",
    "    return segmented_data"
   ],
   "id": "4f61aabc86cfaa15",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:40:18.916806Z",
     "start_time": "2024-09-12T11:40:18.039287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for item in preprocessed_dataset:\n",
    "    ecg_data = item['data']  # ECG 데이터 가져오기\n",
    "    segmented_ecg = segment_ecg(ecg_data)  # 분할된 데이터\n",
    "    item['segmented_data'] = segmented_ecg  # 원래 데이터에 'segmented_data' 추가"
   ],
   "id": "1e49dd5a7ef3f076",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:40:48.659727Z",
     "start_time": "2024-09-12T11:40:48.654727Z"
    }
   },
   "cell_type": "code",
   "source": "len(preprocessed_dataset[0]['segmented_data'])",
   "id": "a1d14e3458ed6e4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:51:50.262986Z",
     "start_time": "2024-09-12T11:51:50.258986Z"
    }
   },
   "cell_type": "code",
   "source": "preprocessed_dataset[0]['label']",
   "id": "af2dbaae4289c175",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T12:37:31.834733Z",
     "start_time": "2024-09-12T12:37:31.818733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, preprocessed_dataset):\n",
    "        \"\"\"\n",
    "        ECG 데이터를 PyTorch Dataset으로 변환하는 클래스.\n",
    "        각 dict의 'data'와 'label' 필드를 쌍으로 반환.\n",
    "        \n",
    "        :param preprocessed_dataset: 분할된 ECG 데이터가 포함된 dict 리스트\n",
    "        \"\"\"\n",
    "        self.data_label_pairs = []\n",
    "\n",
    "        # 각 데이터에 대해 분할된 segment와 label을 쌍으로 저장\n",
    "        for item in preprocessed_dataset:\n",
    "            ecg_data = item['data']  # ECG 데이터\n",
    "            label = item['label']    # 레이블 (예: 정상, 비정상 등)\n",
    "            segmented_ecg = segment_ecg(ecg_data)  # ECG 데이터를 분할\n",
    "\n",
    "            # 분할된 segment와 label을 쌍으로 저장\n",
    "            for segment in segmented_ecg:\n",
    "                self.data_label_pairs.append((segment, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"데이터셋의 전체 길이를 반환\"\"\"\n",
    "        return len(self.data_label_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        인덱스에 해당하는 (segment, label) 쌍을 반환. PyTorch Dataloader가 이 메서드를 호출함.\n",
    "        데이터를 텐서로 변환하여 반환.\n",
    "        \"\"\"\n",
    "        ecg_segment, label = self.data_label_pairs[idx]\n",
    "        \n",
    "        ecg_tensor = torch.permute(torch.tensor(ecg_segment, dtype=torch.float32),(1,0))\n",
    "        if label == 'N':\n",
    "            label = 0\n",
    "        elif label ==  'A':\n",
    "            label = 1\n",
    "        elif label == 'O':\n",
    "            label = 2\n",
    "        elif label ==  '~' :\n",
    "            label = 3\n",
    "        \n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)  # 레이블은 보통 정수형\n",
    "        return ecg_tensor, label_tensor\n"
   ],
   "id": "862a47e3a6bf2d82",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T12:37:33.486514Z",
     "start_time": "2024-09-12T12:37:32.860458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ecg_dataset = ECGDataset(preprocessed_dataset)\n",
    "\n",
    "# DataLoader로 변환하여 배치 단위로 데이터를 가져오도록 설정\n",
    "dataloader = DataLoader(ecg_dataset, batch_size=32, shuffle=True)\n"
   ],
   "id": "69d5f955e21f20d4",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T12:37:33.502515Z",
     "start_time": "2024-09-12T12:37:33.492515Z"
    }
   },
   "cell_type": "code",
   "source": "len(ecg_dataset.__getitem__(1)[0])",
   "id": "c0c9a286afb0e120",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for batch in dataloader:\n",
    "    ecg_segments, labels = batch\n",
    "    print(ecg_segments.shape)  # 각 배치의 ECG 데이터 텐서 크기 출력\n",
    "    print(labels.shape)  "
   ],
   "id": "db6a06b7791af4c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T12:37:48.730936Z",
     "start_time": "2024-09-12T12:37:48.713936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "input_length = 342\n",
    "class ECGCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(ECGCNN, self).__init__()\n",
    "        # 1D Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=16, kernel_size=7, stride=1, padding=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "        # Pooling layers\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * (input_length // 8), 128)  # input_length는 각 segment의 길이\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1D Convolution + Activation + Pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, num_features)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "91fafaa6afd3d4b0",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T12:37:49.736920Z",
     "start_time": "2024-09-12T12:37:49.710659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LEARNING_RATE = 0.001\n",
    "ecgcnn = ECGCNN().to('cuda')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimzer = torch.optim.Adam(ecgcnn.parameters(), lr=LEARNING_RATE)\n"
   ],
   "id": "61e2fec1081fd6bc",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T13:04:39.035315Z",
     "start_time": "2024-09-12T12:39:47.718215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device='cuda'\n",
    "num_epochs=100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for ecg_segments, labels in dataloader:\n",
    "        ecg_segments, labels = ecg_segments.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = ecgcnn(ecg_segments)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ],
   "id": "1e5a814871b45e3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.7754\n",
      "Epoch [2/100], Loss: 1.0019\n",
      "Epoch [3/100], Loss: 0.1630\n",
      "Epoch [4/100], Loss: 0.7728\n",
      "Epoch [5/100], Loss: 1.2270\n",
      "Epoch [6/100], Loss: 0.1316\n",
      "Epoch [7/100], Loss: 0.1066\n",
      "Epoch [8/100], Loss: 0.9323\n",
      "Epoch [9/100], Loss: 1.0292\n",
      "Epoch [10/100], Loss: 0.5446\n",
      "Epoch [11/100], Loss: 0.1132\n",
      "Epoch [12/100], Loss: 0.0286\n",
      "Epoch [13/100], Loss: 0.2972\n",
      "Epoch [14/100], Loss: 0.2895\n",
      "Epoch [15/100], Loss: 0.0863\n",
      "Epoch [16/100], Loss: 0.1750\n",
      "Epoch [17/100], Loss: 0.0076\n",
      "Epoch [18/100], Loss: 0.1402\n",
      "Epoch [19/100], Loss: 0.0736\n",
      "Epoch [20/100], Loss: 0.3066\n",
      "Epoch [21/100], Loss: 0.5503\n",
      "Epoch [22/100], Loss: 0.1271\n",
      "Epoch [23/100], Loss: 0.3150\n",
      "Epoch [24/100], Loss: 1.2040\n",
      "Epoch [25/100], Loss: 0.3678\n",
      "Epoch [26/100], Loss: 0.1458\n",
      "Epoch [27/100], Loss: 0.3943\n",
      "Epoch [28/100], Loss: 0.1789\n",
      "Epoch [29/100], Loss: 0.0101\n",
      "Epoch [30/100], Loss: 0.0113\n",
      "Epoch [31/100], Loss: 0.0018\n",
      "Epoch [32/100], Loss: 0.0188\n",
      "Epoch [33/100], Loss: 0.0064\n",
      "Epoch [34/100], Loss: 0.0005\n",
      "Epoch [35/100], Loss: 0.8232\n",
      "Epoch [36/100], Loss: 0.2870\n",
      "Epoch [37/100], Loss: 0.0000\n",
      "Epoch [38/100], Loss: 0.0722\n",
      "Epoch [39/100], Loss: 0.2537\n",
      "Epoch [40/100], Loss: 0.0455\n",
      "Epoch [41/100], Loss: 0.7374\n",
      "Epoch [42/100], Loss: 0.2289\n",
      "Epoch [43/100], Loss: 0.0045\n",
      "Epoch [44/100], Loss: 0.0201\n",
      "Epoch [45/100], Loss: 0.0017\n",
      "Epoch [46/100], Loss: 1.7625\n",
      "Epoch [47/100], Loss: 0.0019\n",
      "Epoch [48/100], Loss: 0.0067\n",
      "Epoch [49/100], Loss: 0.0000\n",
      "Epoch [50/100], Loss: 0.0004\n",
      "Epoch [51/100], Loss: 0.0000\n",
      "Epoch [52/100], Loss: 0.4169\n",
      "Epoch [53/100], Loss: 0.0002\n",
      "Epoch [54/100], Loss: 0.5075\n",
      "Epoch [55/100], Loss: 0.0226\n",
      "Epoch [56/100], Loss: 0.0089\n",
      "Epoch [57/100], Loss: 0.0001\n",
      "Epoch [58/100], Loss: 0.0055\n",
      "Epoch [59/100], Loss: 0.0022\n",
      "Epoch [60/100], Loss: 0.3299\n",
      "Epoch [61/100], Loss: 0.0113\n",
      "Epoch [62/100], Loss: 0.0000\n",
      "Epoch [63/100], Loss: 0.0391\n",
      "Epoch [64/100], Loss: 0.0000\n",
      "Epoch [65/100], Loss: 0.0366\n",
      "Epoch [66/100], Loss: 0.9879\n",
      "Epoch [67/100], Loss: 0.0000\n",
      "Epoch [68/100], Loss: 0.0643\n",
      "Epoch [69/100], Loss: 0.0000\n",
      "Epoch [70/100], Loss: 0.1967\n",
      "Epoch [71/100], Loss: 0.0016\n",
      "Epoch [72/100], Loss: 0.0120\n",
      "Epoch [73/100], Loss: 0.0019\n",
      "Epoch [74/100], Loss: 0.0012\n",
      "Epoch [75/100], Loss: 0.0002\n",
      "Epoch [76/100], Loss: 0.0013\n",
      "Epoch [77/100], Loss: 0.0000\n",
      "Epoch [78/100], Loss: 0.0000\n",
      "Epoch [79/100], Loss: 0.0095\n",
      "Epoch [80/100], Loss: 0.0013\n",
      "Epoch [81/100], Loss: 0.0007\n",
      "Epoch [82/100], Loss: 0.0008\n",
      "Epoch [83/100], Loss: 0.0000\n",
      "Epoch [84/100], Loss: 0.1540\n",
      "Epoch [85/100], Loss: 0.0013\n",
      "Epoch [86/100], Loss: 0.0000\n",
      "Epoch [87/100], Loss: 0.0126\n",
      "Epoch [88/100], Loss: 0.0000\n",
      "Epoch [89/100], Loss: 0.0002\n",
      "Epoch [90/100], Loss: 0.0000\n",
      "Epoch [91/100], Loss: 0.0004\n",
      "Epoch [92/100], Loss: 0.0002\n",
      "Epoch [93/100], Loss: 0.0001\n",
      "Epoch [94/100], Loss: 0.0000\n",
      "Epoch [95/100], Loss: 0.0000\n",
      "Epoch [96/100], Loss: 0.0000\n",
      "Epoch [97/100], Loss: 0.0024\n",
      "Epoch [98/100], Loss: 0.0000\n",
      "Epoch [99/100], Loss: 0.0094\n",
      "Epoch [100/100], Loss: 0.0000\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "290bd2da69f9d491"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
